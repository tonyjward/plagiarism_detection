{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a PyTorch Classifier\n",
    "In the udacity course we were introduced to a plagiarism dataset which we will use here to train our model. Rather than go through the feature engineering steps we'll simply take the training and testing data from that notebook as our starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save torch model output\n",
    "MODEL_DIR = 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "plagiarism_data = pickle.load(open('../udacity/plagiarism_data.p', 'rb'))\n",
    "plagiarism_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack data\n",
    "train_x, train_y, test_x, test_y = plagiarism_data.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "print(f\"train_x has shape {train_x.shape}\")\n",
    "print(f\"train_y has shape {train_y.shape}\")\n",
    "print(f\"test_x has shape {test_x.shape}\")\n",
    "print(f\"test_y has shape {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Training Data\n",
    "Visualising our training data may give an indication as to the relationship between the inputs and the targets, and guide us to the level of non-linearity present in the data. It will inform our choice of algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (8, 5))\n",
    "plt.scatter(train_x[:,1], train_y)\n",
    "plt.title('Longest Common Subsequence vs Plagiarism')\n",
    "plt.xlabel('c20')\n",
    "plt.ylabel('plagiarism')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything above longest common subsequence 0.38 appears to be plagiarised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "plt.scatter(train_x[:,0], train_y)\n",
    "plt.title('Containmnet vs Plagiarism')\n",
    "plt.xlabel('c20')\n",
    "plt.ylabel('plagiarism')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Containmnet looks to be a much better predictor of plagiarism, with everything above 0 indicating plagiarism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points are coloured by class, train_y\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (8, 5))\n",
    "plt.scatter(train_x[:,0], train_x[:,1], c = train_y)\n",
    "plt.title('Two Way Scatter Plot')\n",
    "plt.xlabel('c20')\n",
    "plt.ylabel('lcs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using both these features in a model should give us a good decision boundary for classifying plagiarism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train a neural network to classify the students answers into plagiarised vs non-plagiarised. As part of the udacity course they provided some boiler plate code to train the network using amazon sagemaker. \n",
    "\n",
    "In our situation we are not looking to deploy the model as an API and therefore using sagemaker is overkill, and too timeconsuming. Instead we'll write a basic pytorch classifier ourselves, and score the medium data using the local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "# Turn the numpy arrays into tensors\n",
    "train_x = torch.from_numpy(train_x).float()\n",
    "train_y = torch.from_numpy(train_y).float().squeeze()\n",
    "\n",
    "# build the torch dataset\n",
    "train_ds = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "\n",
    "# build the data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, criterion, device):\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # monitor training loss\n",
    "        train_loss_total = 0\n",
    "        \n",
    "        ######################\n",
    "        # train the model    #\n",
    "        ######################\n",
    "        model.train() # prep model for training\n",
    "        \n",
    "        for data, target in train_loader:\n",
    "            target = target.contiguous().view(-1, 1)\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad() # zero accumulated gradients\n",
    "            output = model(data) # make a forward pass\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward() # make a backward pass\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_total += loss.item()\n",
    "            \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss_total / len(train_loader)\n",
    "        \n",
    "        # print loss statistics\n",
    "        print(f\"Epoch: {epoch}, train_loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a neural network that performs binary classification.\n",
    "    The network should accept your number of features as input, and produce \n",
    "    a single sigmoid value, that can be rounded to a label: 0 or 1, as output.\n",
    "    \n",
    "    Notes on training:\n",
    "    To train a binary classifier in PyTorch, use BCELoss.\n",
    "    BCELoss is binary cross entropy loss, documentation: https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss\n",
    "    \"\"\"\n",
    "\n",
    "    ## TODO: Define the init function, the input params are required (for loading code in train.py to work)\n",
    "    def __init__(self, input_features, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up linear layers.\n",
    "        Use the input parameters to help define the layers of your model.\n",
    "        :param input_features: the number of input features in your training/test data\n",
    "        :param hidden_dim: helps define the number of nodes in the hidden layer(s)\n",
    "        :param output_dim: the number of outputs you want to produce\n",
    "        \"\"\"\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "\n",
    "        # define any initial layers, here\n",
    "        self.fc1 = nn.Linear(input_features, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # sigmoid layer\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    ## TODO: Define the feedforward behavior of the network\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on input features, x.\n",
    "        :param x: A batch of input features of size (batch_size, input_features)\n",
    "        :return: A single, sigmoid-activated value as output\n",
    "        \"\"\"\n",
    "        \n",
    "        # define the feedforward behavior\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sig(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_features = 2\n",
    "hidden_dim = 7\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "epochs = 300\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BinaryClassifier(input_features, hidden_dim, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "train(model, train_dl, epochs, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided predict function\n",
    "def predict_fn(input_data, model):\n",
    "    print('Predicting class probabilities for the input data...')\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Process input_data so that it is ready to be sent to our model.\n",
    "    data = torch.from_numpy(input_data.astype('float32'))\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Predicted scores\n",
    "    probabilities = model(data).cpu().detach().numpy()\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = predict_fn(test_x, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = probabilities.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a variety of model metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(test_preds, test_labels, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a test set given the prediction endpoint.  \n",
    "    Return binary classification metrics.\n",
    "    :param predictor: A prediction endpoint\n",
    "    :param test_features: Test features\n",
    "    :param test_labels: Class labels for test data\n",
    "    :param verbose: If True, prints a table of all performance metrics\n",
    "    :return: A dictionary of performance metrics.\n",
    "    \"\"\"\n",
    "    test_preds = np.squeeze(test_preds)\n",
    "    \n",
    "    # calculate true positives, false positives, true negatives, false negatives\n",
    "    tp = np.logical_and(test_labels, test_preds).sum()\n",
    "    fp = np.logical_and(1-test_labels, test_preds).sum()\n",
    "    tn = np.logical_and(1-test_labels, 1-test_preds).sum()\n",
    "    fn = np.logical_and(test_labels, 1-test_preds).sum()\n",
    "    \n",
    "    # calculate binary classification metrics\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    \n",
    "    # print metrics\n",
    "    if verbose:\n",
    "        print(pd.crosstab(test_labels, test_preds, rownames=['actuals'], colnames=['predictions']))\n",
    "        print(\"\\n{:<11} {:.3f}\".format('Recall:', recall))\n",
    "        print(\"{:<11} {:.3f}\".format('Precision:', precision))\n",
    "        print(\"{:<11} {:.3f}\".format('Accuracy:', accuracy))\n",
    "        print()\n",
    "        \n",
    "    return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn, \n",
    "            'Precision': precision, 'Recall': recall, 'Accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate(labels, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: complete in the model_info by adding three argument names, the first is given\n",
    "# Keep the keys of this dictionary as they are \n",
    "import os\n",
    "\n",
    "model_info_path = os.path.join(MODEL_DIR, 'model_info.pth')\n",
    "with open(model_info_path, 'wb') as f:\n",
    "    model_info = {\n",
    "        'input_features': input_features,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'output_dim': output_dim,\n",
    "    }\n",
    "    torch.save(model_info, f)\n",
    "  \n",
    "# Save the model parameters\n",
    "model_path = os.path.join(MODEL_DIR, 'model.pth')\n",
    "with open(model_path, 'wb') as f:\n",
    "    torch.save(model.cpu().state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
